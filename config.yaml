env:
  life:
    name: 'Life-v0'
    update_rate: 1
    n_agents: 1

  cart:
    name: 'CartPole-v0'

  mountain:
    name: 'MountainCar-v0'

  beam:
    name: 'BeamRider-v0'

agents:
  social:
    controller_name: SocialCRAController
    agent_name: SocialCRAAgent
#    update_threshold: 1
    ac_network: ConceptNetwork
    n_concepts: 20

    td_step: 10
    entropy_coef: .01
    ac_cfg:
      n_concepts: 20
      lr: .01
      model_size: 32

  cra:
    controller_name: CRAController
    agent_name: CRAAgent
    actor_network: ChannelNetwork # CRANetwork ActorFCNetwork ACNetwork ACENetwork ChannelNetwork
    critic_network: CriticFCNetworks # CriticFCNetworks CENetwork
    td_step: -1
    supervised_loss: False
    entropy_coef: .005
    discount: True
    actor_cfg:
      lr: .01
      model_size: 64

  ccra:
    controller_name: CRAController
    agent_name: CRAAgent
    actor_network: ChannelNetworkSwitch # ChannelNetworkSwitch ChannelNetwork ChannelNetworkBasic CRANetwork ActorFCNetwork ACNetwork ACENetwork ChannelNetwork
    critic_network: CriticChannels # CriticFCNetwork CENetwork CriticChannels
    condition_on_action: True

    td_step: -1
    supervised_loss: False
    entropy_coef: 0.01
    discount: True
    actor_cfg:
      state_embedding_size: 10
      lr: .001
      model_size: 32
      gradient_clip: 2.0

  exp:
    controller_name: EXPController
    agent_name: ExpAgent
    actor_network: ActorFCNetwork
    critic_network: CriticFCNetwork
    reward_network: Attention
    discount_factor: .99
    entropy_coef: .05
    supervised_loss: False
    attention_reward: True
    td_step: -1
    actor_cfg:
      lr: .001
      model_size: 64
      gradient_clip: 1.5
    critic_cfg:
      lr: .001
      model_size: 64
      gradient_clip: 1.5

  a2c:
    controller_name: ACController
    agent_name: A2CAgent
    actor_network: ActorFCNetwork
    critic_network: CriticFCNetwork
    discount_factor: .99
    entropy_coef: .05
    supervised_loss: False
    td_step: 10
    actor:
      lr: .01
      model_size: 64
      gradient_clip: 1.5
    critic:
      lr: .01
      model_size: 64
      gradient_clip: 1.5
  pg:
    controller_name: PGController
    agent_name: PGAgent
    actor_network: ActorFCNetwork
    discount_factor: .99
    entropy_coef: .05
    actor:
      lr: .01
      model_size: 64

training:
  variation: ''
  n_episodes: 2000 #
  timeout: 2000 # steps in one episode before time out, unless not episodic
  eval_rewards_window: 20
