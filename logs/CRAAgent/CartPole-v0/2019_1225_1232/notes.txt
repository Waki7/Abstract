
@register_network
class ChannelNetworkBasic(BaseNetwork):
    def __init__(self, n_features, out_shape, cfg,
                 in_channels=None, in_shapes=None, out_channels=None, out_shapes=None, **kwargs):
        super(ChannelNetworkBasic, self).__init__(cfg)
        ##########################################################################################
        # set cfg parameters
        ##########################################################################################
        reward_embedding_size = cfg.get('reward_embedding_size', 1)
        self.model_size = cfg.get('model_size', 32)
        self.bias = cfg.get('bias', True)
        self.out_shape = out_shape

        ##########################################################################################
        # define channels
        ##########################################################################################
        super().__init__(cfg)
        self.n_actions = out_shape
        self.linear1 = nn.Linear(n_features + out_shape, self.model_size)
        self.linear2 = nn.Linear(self.model_size, out_shape)
        self.hidden_state = torch.zeros((1, out_shape)).to(settings.DEVICE)
        self.create_optimizer()

    def forward(self, env_input, **kwargs):
        x = torch.cat([env_input, self.hidden_state.detach()], dim=-1)
        x = F.leaky_relu(self.linear1(x))
        x = F.softmax(self.linear2(x), dim=-1)
        self.hidden_state = x
        return x, None

    def prune(self):
        self.hidden_state = torch.zeros((1, self.out_shape)).to(settings.DEVICE)

